{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helpers' from '/home/ubuntu/joanna/VAE-IR/src/helpers.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import external tools:\n",
    "import time \n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchsummary import summary\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# I am running this script on two different computers, so i need to change paths\n",
    "# depending on computer in use: \n",
    "if getpass.getuser()==\"joanna.luberadzka\":\n",
    "    projectdir=\"/Users/joanna.luberadzka/Documents/VAE-IR/\"\n",
    "    datadir=\"/Users/joanna.luberadzka/Documents/Data/IR_Arni_upload_numClosed_0-5/\"\n",
    "elif getpass.getuser()==\"ubuntu\":\n",
    "    projectdir=\"/home/ubuntu/joanna/VAE-IR/\"\n",
    "    datadir=\"/home/ubuntu/Data/IR_Arni_upload_numClosed_0-5/\"\n",
    "\n",
    "# Add path of this project\n",
    "sys.path.insert(0, projectdir+'src/')\n",
    "\n",
    "# Import and automatically reload my own modules:\n",
    "import models; importlib.reload(models)\n",
    "import train; importlib.reload(train)\n",
    "import datasetprep as dsprep; importlib.reload(dsprep)\n",
    "import helpers; importlib.reload(helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.07 GiB (GPU 0; 16.00 GiB total capacity; 0 bytes already allocated; 402.05 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# load trained model:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model\u001b[39m=\u001b[39mmodels\u001b[39m.\u001b[39mVAE_Lin(x_len\u001b[39m=\u001b[39m\u001b[39m24000\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(projectdir \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mmodels/trained_model_01-02-2023--12-17.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# put the model in evaluation mode\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/joanna/venvs/testenv/lib/python3.8/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/joanna/venvs/testenv/lib/python3.8/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1051\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/joanna/venvs/testenv/lib/python3.8/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1021\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/joanna/venvs/testenv/lib/python3.8/site-packages/torch/serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    997\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39m_UntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[39m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39m_TypedStorage(\n\u001b[0;32m-> 1001\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1002\u001b[0m     dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/joanna/venvs/testenv/lib/python3.8/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    176\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/joanna/venvs/testenv/lib/python3.8/site-packages/torch/serialization.py:157\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_UntypedStorage(obj\u001b[39m.\u001b[39mnbytes(), device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(location))\n\u001b[1;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39;49mcuda(device)\n",
      "File \u001b[0;32m~/joanna/venvs/testenv/lib/python3.8/site-packages/torch/_utils.py:78\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m new_type(indices, values, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_UntypedStorage(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize(), device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m))\u001b[39m.\u001b[39mcopy_(\u001b[39mself\u001b[39m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.07 GiB (GPU 0; 16.00 GiB total capacity; 0 bytes already allocated; 402.05 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "# load trained model:\n",
    "model=models.VAE_Lin(x_len=24000).to(\"cpu\")\n",
    "model.load_state_dict(torch.load(projectdir + \"models/trained_model_01-02-2023--12-17.pth\")).to(\"cpu\")\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# data:\n",
    "if getpass.getuser()==\"joanna.luberadzka\":\n",
    "    INFO_FILE = projectdir + \"irstats_ARNIandBUT_datura.csv\"\n",
    "elif getpass.getuser()==\"ubuntu\":\n",
    "    INFO_FILE = projectdir+\"irstats_ARNIandBUT_datura.csv\"\n",
    "\n",
    "SAMPLING_RATE=8e3\n",
    "# instantiate data set \n",
    "dataset = dsprep.DatasetRirs(INFO_FILE,SAMPLING_RATE)\n",
    "\n",
    "\n",
    "# *** Encoding: Using a trained variational autoencoder model, \n",
    "# generate a lower-dimensional embedding for each impulse response.\n",
    "\n",
    "embeddings_mu=[] # list for storing ir embeddings\n",
    "embeddings_rt=[] # list for storing rt values\n",
    "embeddings_drr=[] # list for storing drr values\n",
    "embeddings_isarni=[] # list for storing bool indicating which database\n",
    "embeddings_edt=[] # list for storing edt vallues\n",
    "embeddings_cte=[] # list for storing cte values\n",
    "\n",
    "# take 100 random irs from the data set\n",
    "ir_rand_indices=random.sample(range(len(dataset)),100)\n",
    "\n",
    "for i in ir_rand_indices:\n",
    "    # get info of an impulse response with a specific index\n",
    "    ir, labels= dataset[i]\n",
    "    # encode the input into mu and sigma (standard in VAE)\n",
    "    mu, sigma = model.encode(ir)\n",
    "    # mu is the embedding (sigma provides additional info about the uncertainty of this embedding)   \n",
    "    emb = mu.squeeze(dim=0) \n",
    "    # convert to numpy array and append the list of embeddings\n",
    "    embeddings_mu.append(emb.detach().cpu().numpy())\n",
    "    embeddings_rt.append(labels[\"rt\"])\n",
    "    embeddings_drr.append(labels[\"drr\"])\n",
    "    embeddings_edt.append(labels[\"edt\"])\n",
    "    embeddings_cte.append(labels[\"cte\"])\n",
    "    embeddings_isarni.append(labels[\"isarni\"])\n",
    "\n",
    "# covert from list of arrays to one array\n",
    "embeddings_mu=np.array(embeddings_mu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.4131025e-06 -4.6789646e-06  2.4437904e-06  3.3527613e-06\n",
      "  8.1956387e-07 -1.8328428e-06  1.4603138e-06 -3.2335520e-06\n",
      " -2.3841858e-06  2.2202730e-06  2.0861626e-06  4.2319298e-06\n",
      " -1.0952353e-06  2.5928020e-06  1.1473894e-06 -1.0728836e-06\n",
      " -7.5623393e-07 -1.1920929e-06  4.9769878e-06 -4.8428774e-06\n",
      "  1.0930002e-05 -1.3411045e-07 -4.3213367e-07 -5.6177378e-06]\n",
      "[ 8.4131025e-06 -4.6789646e-06  2.4437904e-06  3.3527613e-06\n",
      "  8.1956387e-07 -1.8328428e-06  1.4603138e-06 -3.2335520e-06\n",
      " -2.3841858e-06  2.2202730e-06  2.0861626e-06  4.2319298e-06\n",
      " -1.0952353e-06  2.5928020e-06  1.1473894e-06 -1.0728836e-06\n",
      " -7.5623393e-07 -1.1920929e-06  4.9769878e-06 -4.8428774e-06\n",
      "  1.0930002e-05 -1.3411045e-07 -4.3213367e-07 -5.6177378e-06]\n",
      "[ 8.4131025e-06 -4.6789646e-06  2.4437904e-06  3.3527613e-06\n",
      "  8.1956387e-07 -1.8328428e-06  1.4603138e-06 -3.2335520e-06\n",
      " -2.3841858e-06  2.2202730e-06  2.0861626e-06  4.2319298e-06\n",
      " -1.0952353e-06  2.5928020e-06  1.1473894e-06 -1.0728836e-06\n",
      " -7.5623393e-07 -1.1920929e-06  4.9769878e-06 -4.8428774e-06\n",
      "  1.0930002e-05 -1.3411045e-07 -4.3213367e-07 -5.6177378e-06]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_mu[1])\n",
    "print(embeddings_mu[2])\n",
    "print(embeddings_mu[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings_mu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# *** Visualization: To visualize each encoding, the D-dimensional embeddings \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# have to be reduced to 2 dimensions. This can be done with two methods: \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# PCA (linear) or TSNE (non-linear)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m embeddings_pca\u001b[39m=\u001b[39mPCA(n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mfit_transform(embeddings_mu)\n\u001b[1;32m      5\u001b[0m embeddings_tsne \u001b[39m=\u001b[39m TSNE(n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, init\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrandom\u001b[39m\u001b[39m'\u001b[39m, perplexity\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m)\u001b[39m.\u001b[39mfit_transform(embeddings_mu)\n\u001b[1;32m      7\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings_mu' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# *** Visualization: To visualize each encoding, the D-dimensional embeddings \n",
    "# have to be reduced to 2 dimensions. This can be done with two methods: \n",
    "# PCA (linear) or TSNE (non-linear)\n",
    "embeddings_pca=PCA(n_components=2).fit_transform(embeddings_mu)\n",
    "embeddings_tsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=30).fit_transform(embeddings_mu)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(embeddings_pca[:,0] , embeddings_pca[:,1])\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(embeddings_pca[:,0] , embeddings_pca[:,1])\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot (2,3,1)\n",
    "plt.scatter(embeddings_tsne[:,0] , embeddings_tsne[:,1], c=embeddings_rt)\n",
    "plt.title('color: rt')\n",
    "plt.subplot (2,3,2)\n",
    "plt.scatter(embeddings_tsne[:,0] , embeddings_tsne[:,1], c=embeddings_drr)\n",
    "plt.title('color: drr')\n",
    "plt.subplot (2,3,3)\n",
    "plt.scatter(embeddings_tsne[:,0] , embeddings_tsne[:,1], c=embeddings_edt)\n",
    "plt.title('color: edt')\n",
    "plt.subplot (2,3,4)\n",
    "plt.scatter(embeddings_tsne[:,0] , embeddings_tsne[:,1], c=embeddings_cte)\n",
    "plt.title('color: cte')\n",
    "plt.subplot (2,3,5)\n",
    "plt.scatter(embeddings_tsne[:,0] , embeddings_tsne[:,1], c=embeddings_isarni)\n",
    "plt.title('color: isarni')\n",
    "\n",
    "\n",
    "\n",
    "# # use pca init for repeatable results\n",
    "# pca      = PCA(n_components=2)\n",
    "# x_reduce = pca.fit_transform(z_df)\n",
    "\n",
    "\n",
    "# emb_tsne = TSNE(n_components=2, learning_rate='auto').fit_transform(embeddings.detach().cpu().numpy())\n",
    "# plt.scatter(emb_tsne[:,0] , emb_tsne[:,1], c=category_mappings)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# pick a random IRs for each IR choose the closest and the furthest embedding from the set of embeddings. \n",
    "\n",
    "# get info of an impulse response with a specific index\n",
    "# print(\"filename= \"+ IrData[\"filepath\"][2])\n",
    "# print(\"rt= \"+ str(IrData[\"rt\"][2]))\n",
    "# print(\"drr= \"+ str(IrData[\"drr\"][2]))\n",
    "\n",
    "# convolve signal with a closest and furthest ir - the one with closest should sound similar and the one with furthest should sound different\n",
    "\n",
    "# y = signal.fftconvolve(x, h, mode = 'full')\n",
    "# sf.write(\"convolved.wav\", y.T, sr_h, subtype='PCM_24')\n",
    "# sf.write(\"anechoic.wav\", x.T, sr_h, subtype='PCM_24')\n",
    "\n",
    "\n",
    "\n",
    "# # plot parameters of impulse responses in the data  \n",
    "# fig, ax = plt.subplots()\n",
    "# ax.scatter(x=IrData[\"rt\"][0:11331], y=IrData[\"drr\"][0:11331],label=\"Arni\", alpha=0.2)\n",
    "# ax.scatter(x=IrData[\"rt\"][11331:], y=IrData[\"drr\"][11331:],label=\"BUT\",alpha=0.2)\n",
    "# plt.xlabel(\"T60\")\n",
    "# plt.ylabel(\"DRR\")\n",
    "# plt.legend()\n",
    "# ax.set(title=\"Two IR data bases\")\n",
    "# plt.show()\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.scatter(x=IrData[\"rt\"], y=IrData[\"drr\"],c=IrData[\"cte\"], alpha=0.2)\n",
    "# sc1=plt.xlabel(\"T60\")\n",
    "# sc2=plt.ylabel(\"DRR\")\n",
    "# ax.set(title=\"Early to late reflections (CTE) \")\n",
    "# plt.show()\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.scatter(x=IrData[\"rt\"], y=IrData[\"drr\"],c=IrData[\"edt\"], alpha=0.2)\n",
    "# plt.xlabel(\"T60\")\n",
    "# plt.ylabel(\"DRR\")\n",
    "# ax.set(title=\"Early decay time (EDT)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info of an impulse response with a specific index\n",
    "filename= IrData[\"filepath\"][1200]\n",
    "# load chosen RIR\n",
    "h, sr_h= torchaudio.load(filename)\n",
    "# load anechoic sound file\n",
    "x, sr_x=torchaudio.load(\"anechoic.wav\")\n",
    "# resample anechoic signal\n",
    "x=torchaudio.transforms.Resample(sr_x,sr_h)(x)\n",
    "# convolve anechoic signal with RIR: y=x*h\n",
    "y = signal.fftconvolve(x, h, mode = 'full')\n",
    "# set signal levels\n",
    "x=x.numpy()\n",
    "x=helpers.set_level(x,-30)\n",
    "y=helpers.set_level(y,-30)\n",
    "# save signals\n",
    "sf.write(\"convolved.wav\", y.T, sr_h, subtype='PCM_24')\n",
    "sf.write(\"anechoic.wav\", x.T, sr_h, subtype='PCM_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio('convolved.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio('anechoic.wav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce963d0b11d3bbec7fb5cbe3b3e5eb22455c53ff6b457d89286ef32149820fa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
