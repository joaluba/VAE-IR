{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import external tools:\n",
    "import time \n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchsummary import summary\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio\n",
    "from datetime import datetime\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "projectdir=\"/home/ubuntu/joanna/VAE-IR/\"\n",
    "datadir=\"/home/ubuntu/Data/\"\n",
    "\n",
    "# Add path of this project\n",
    "sys.path.insert(0, projectdir+'src/')\n",
    "\n",
    "# # Import and automatically reload my own modules:\n",
    "import sig2ir_datasetprep as dsprep;importlib.reload(dsprep)\n",
    "import helpers; importlib.reload(helpers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivation: In VR/XR it might help to change the room acoustic properties of sound recorded in one space, so that it sounds as if it came from another space. This problem can be devided into two sub-tasks where the first one is to de-reverberate a signal and the second one is to convolve the de-reverberated signal with a new room impulse response to obtain a perceptually different space. However, the desired room impulse response is usually not explicitly given, so a part of the problem is to estimate a room impulse response from a reverberant signal recorded in a target acoustic space. \n",
    "\n",
    "Task: Given a mono, reverberant signal, estimate the room impulse response. \n",
    "\n",
    "Approach: Use deep learning to solve that problem. Use encoder-decoder architecture with the waveform at the input and room impulse response at the output. \n",
    "\n",
    "Data: Speech convolved with various impulse responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame containing a list of all available audio files (will be used as audio pool in the dataset generation)\n",
    "audiodatadir=datadir + \"VCTK/wav48_silence_trimmed/\"\n",
    "filename=\"audio_VCTK_datura.csv\"\n",
    "\n",
    "import os\n",
    "file_paths = []\n",
    "\n",
    "# Traverse the directory and find all .flac files in subdirectories\n",
    "for root, dirs, files in os.walk(audiodatadir):\n",
    "    for file in files:\n",
    "        if file.endswith('.flac'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "# Create a DataFrame using the file_paths list\n",
    "df_audio = pd.DataFrame({'filepath_sig': file_paths})\n",
    "df_audio.loc[df_audio[\"filepath_sig\"].str.contains(\"VCTK\"),\"database_sig\"]=\"vctk\"\n",
    "\n",
    "df_audio.loc[df_audio[\"filepath_sig\"].str.contains(\"mic1\"),\"mic\"]=1\n",
    "df_audio.loc[df_audio[\"filepath_sig\"].str.contains(\"mic2\"),\"mic\"]=2\n",
    "\n",
    "df_audio.to_csv(\"../\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some small changes in the dataframe storing impulse responses \n",
    "df_ir = pd.read_csv(\"../irstats_ARNIandBUT_datura.csv\",index_col=0)\n",
    "df_ir = df_ir.rename(columns={'filepath': 'filepath_ir'})\n",
    "df_ir.loc[df_ir[\"filepath_ir\"].str.contains(\"Arni\"),\"database_ir\"]=\"arni\"\n",
    "df_ir.loc[df_ir[\"filepath_ir\"].str.contains(\"BUT\"),\"database_ir\"]=\"but\"\n",
    "df_ir.to_csv(\"../irstats_ARNIandBUT_datura.csv\")\n",
    "df_ir.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for NumPy, Pandas, and PyTorch\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# set up sources of RIRs and audios for dataset\n",
    "AUDIO_INFO_FILE = \"/home/ubuntu/joanna/VAE-IR/audio_VCTK_datura.csv\"\n",
    "IR_INFO_FILE = \"/home/ubuntu/joanna/VAE-IR/irstats_ARNIandBUT_datura.csv\"\n",
    "SAMPLING_RATE=48e3\n",
    "\n",
    "df_audiopool=pd.read_csv(AUDIO_INFO_FILE,index_col=0)\n",
    "df_irs=pd.read_csv(IR_INFO_FILE,index_col=0)\n",
    "df_irs=df_irs[df_irs[\"database_ir\"]==\"arni\"]\n",
    "df_irs=df_irs.head(10)\n",
    "\n",
    "dataset=dsprep.Dataset_SpeechInSpace(df_audiopool,df_irs,N_per_ir=1000)\n",
    "\n",
    "# # create a tag for dataset info file\n",
    "# current_datetime = datetime.now()\n",
    "# nametag = current_datetime.strftime(\"%d-%m-%Y_%H-%M\")\n",
    "# # save info about dataset\n",
    "# dataset.dataset_info(nametag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a data point: \n",
    "\n",
    "sig, ir, dp, label=dataset[3000]\n",
    "print(label[\"filepath_ir\"])\n",
    "\n",
    "# fig = plt.figure(figsize=(10, 4))\n",
    "# plt.subplot(1,3,1)\n",
    "# plt.plot(ir.T)\n",
    "# plt.xlim((0,200000))\n",
    "# plt.ylim((-1,1))\n",
    "# plt.subplot(1,3,2)\n",
    "# plt.plot(sig.T)\n",
    "# plt.xlim((0,200000))\n",
    "# plt.ylim((-1,1))\n",
    "# plt.subplot(1,3,3)\n",
    "# plt.plot(dp.squeeze([0]).T)\n",
    "# plt.xlim((0,200000))\n",
    "# plt.ylim((-1,1))\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAYBACK: ROOM IMPULSE RESPONSE\n",
    "print(label[\"filepath_ir\"])\n",
    "Audio(ir,rate=48000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAYBACK: EXCERPT FROM SPEECH DATABASE\n",
    "print(label[\"filepath_sig\"])\n",
    "Audio(sig,rate=48000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAYBACK: REVERBERANT SIGNAL \n",
    "Audio(dp.squeeze([0]),rate=48000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
