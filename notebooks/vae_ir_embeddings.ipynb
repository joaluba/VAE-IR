{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import external tools:\n",
    "import time \n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchsummary import summary\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# I am running this script on two different computers, so i need to change paths\n",
    "# depending on computer in use: \n",
    "if getpass.getuser()==\"joanna.luberadzka\":\n",
    "    projectdir=\"/Users/joanna.luberadzka/Documents/VAE-IR/\"\n",
    "    datadir=\"/Users/joanna.luberadzka/Documents/Data/IR_Arni_upload_numClosed_0-5/\"\n",
    "elif getpass.getuser()==\"ubuntu\":\n",
    "    projectdir=\"/home/ubuntu/joanna/VAE-IR/\"\n",
    "    datadir=\"/home/ubuntu/Data/IR_Arni_upload_numClosed_0-5/\"\n",
    "\n",
    "# Add path of this project\n",
    "sys.path.insert(0, projectdir+'src/')\n",
    "\n",
    "# Import and automatically reload my own modules:\n",
    "import models; importlib.reload(models)\n",
    "import train; importlib.reload(train)\n",
    "import datasetprep as dsprep; importlib.reload(dsprep)\n",
    "import helpers; importlib.reload(helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data:\n",
    "if getpass.getuser()==\"joanna.luberadzka\":\n",
    "    INFO_FILE = projectdir + \"irstats_ARNIandBUT_datura.csv\"\n",
    "elif getpass.getuser()==\"ubuntu\":\n",
    "    INFO_FILE = projectdir+\"irstats_ARNIandBUT_datura.csv\"\n",
    "\n",
    "SAMPLING_RATE=8e3\n",
    "# instantiate data set \n",
    "dataset = dsprep.DatasetRirs(INFO_FILE,SAMPLING_RATE,preproc=\"wave\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load trained model and plot example:\n",
    "\n",
    "# ----------- Option 1 : Waveform-to-waveform variational autoencoder with linear layers only ------------\n",
    "model=models.VAE_Lin(x_len=24000).to(\"cpu\")\n",
    "model.load_state_dict(torch.load(projectdir + \"models/trained_model_WAVEA_01-02-2023--12-17.pth\",map_location=\"cpu\"))\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# ----------- Option 2 : Waveform-to-waveform variational autoencoder with 1d-convolutional layers ------------\n",
    "# model=models.Conv1D_VAE(x_len=24000,h_len=256,z_len=24).to(\"cpu\")\n",
    "# model.load_state_dict(torch.load(projectdir + \"models/trained_model_WAVEB_08-02-2023--00-56.pth\",map_location=\"cpu\"))\n",
    "# # put the model in evaluation mode\n",
    "# model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encoding: Using a trained variational autoencoder model, \n",
    "# generate a lower-dimensional embedding for each impulse response.\n",
    "\n",
    "embeddings_mu=[] # list for storing ir embeddings\n",
    "embeddings_rt=[] # list for storing rt values\n",
    "embeddings_drr=[] # list for storing drr values\n",
    "embeddings_isarni=[] # list for storing bool indicating which database\n",
    "embeddings_edt=[] # list for storing edt vallues\n",
    "embeddings_cte=[] # list for storing cte values\n",
    "\n",
    "# take 100 random irs from the data set\n",
    "ir_rand_indices=random.sample(range(len(dataset)),100)\n",
    "\n",
    "for i in ir_rand_indices:\n",
    "    # get info of an impulse response with a specific index\n",
    "    ir, labels= dataset[i]\n",
    "    # encode the input into mu and sigma (standard in VAE)\n",
    "    mu, sigma = model.encode(ir)\n",
    "    # mu is the embedding (sigma provides additional info about the uncertainty of this embedding)   \n",
    "    emb = mu.squeeze(dim=0) \n",
    "    # convert to numpy array and append the list of embeddings\n",
    "    embeddings_mu.append(emb.detach().cpu().numpy())\n",
    "    embeddings_rt.append(labels[\"rt\"])\n",
    "    embeddings_drr.append(labels[\"drr\"])\n",
    "    embeddings_edt.append(labels[\"edt\"])\n",
    "    embeddings_cte.append(labels[\"cte\"])\n",
    "    embeddings_isarni.append(labels[\"isarni\"])\n",
    "\n",
    "# covert from list of arrays to one array\n",
    "embeddings_mu=np.array(embeddings_mu)\n",
    "embeddings_mu=np.squeeze(embeddings_mu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualization: To visualize each encoding, the D-dimensional embeddings \n",
    "# have to be reduced to 2 dimensions. This can be done with two methods: \n",
    "# PCA (linear) or TSNE (non-linear)\n",
    "embeddings_pca=PCA(n_components=2).fit_transform(embeddings_mu)\n",
    "embeddings_tsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=30).fit_transform(embeddings_mu)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(embeddings_pca[:,0] , embeddings_pca[:,1],c=embeddings_rt)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(embeddings_pca[:,0] , embeddings_pca[:,1],c=embeddings_drr)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot (2,3,1)\n",
    "plt.scatter(embeddings_tsne[:,0] , embeddings_tsne[:,1], c=embeddings_rt)\n",
    "plt.title('color: rt')\n",
    "plt.subplot (2,3,2)\n",
    "plt.scatter(embeddings_tsne[:,0] , embeddings_tsne[:,1], c=embeddings_drr)\n",
    "plt.title('color: drr')\n",
    "plt.subplot (2,3,3)\n",
    "plt.scatter(embeddings_tsne[:,0] , embeddings_tsne[:,1], c=embeddings_edt)\n",
    "plt.title('color: edt')\n",
    "plt.subplot (2,3,4)\n",
    "plt.scatter(embeddings_tsne[:,0] , embeddings_tsne[:,1], c=embeddings_cte)\n",
    "plt.title('color: cte')\n",
    "plt.subplot (2,3,5)\n",
    "plt.scatter(embeddings_tsne[:,0] , embeddings_tsne[:,1], c=embeddings_isarni)\n",
    "plt.title('color: isarni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction: Comparing original and reconstructed impulse response (visually)\n",
    "\n",
    "rand_ir_idx=np.random.randint(len(dataset))\n",
    "# get info of an impulse response with a random index\n",
    "ir_orig, labels= dataset[rand_ir_idx]\n",
    "# encode the input into mu and sigma (standard in VAE)\n",
    "ir_recon, mu, sigma=model(ir_orig)  \n",
    "\n",
    "ir_orig_plot=ir_orig.squeeze(1).numpy().T\n",
    "ir_recons_plot=ir_recon.detach().squeeze(1).numpy().T\n",
    "\n",
    "plt.figure(figsize=(10,2))\n",
    "plt.subplot (1,2,1)\n",
    "plt.plot(ir_orig_plot)\n",
    "plt.title(f'Original RIR sample {rand_ir_idx}')\n",
    "plt.subplot (1,2,2)\n",
    "plt.plot(ir_recons_plot)\n",
    "plt.title(f'Reconstructed RIR sample {rand_ir_idx}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction: Look at the spectrum of the original and reconstructed IRs, filter the additional undesired tones\n",
    "IR_orig = np.fft.fft2(ir_orig_plot)\n",
    "IR_recons = np.fft.fft2(ir_recons_plot)\n",
    "\n",
    "N = ir_orig_plot.shape[0];df = 1.0 / 8e3; freqs_axis = np.linspace(0.0, 1.0/(2.0*df), N//2)\n",
    "\n",
    "plt.figure(figsize=(10,2))\n",
    "plt.subplot (1,2,1)\n",
    "plt.plot(freqs_axis, 2.0/N * np.abs(IR_orig[:N//2]))\n",
    "plt.title(f'Original RIR sample {rand_ir_idx}')\n",
    "plt.subplot (1,2,2)\n",
    "plt.plot(freqs_axis, 2.0/N * np.abs(IR_recons[:N//2]))\n",
    "plt.title(f'Reconstructed RIR sample {rand_ir_idx}')\n",
    "plt.show()\n",
    "\n",
    "IR_recons[np.where(2.0/N*np.abs(IR_recons)> 0.02)]=0\n",
    "\n",
    "plt.figure(figsize=(10,2))\n",
    "plt.subplot (1,2,1)\n",
    "plt.plot(freqs_axis, 2.0/N * np.abs(IR_orig[:N//2]))\n",
    "plt.title(f'Original RIR sample {rand_ir_idx}')\n",
    "plt.subplot (1,2,2)\n",
    "plt.plot(freqs_axis, 2.0/N * np.abs(IR_recons[:N//2]))\n",
    "plt.title(f'Reconstructed RIR sample {rand_ir_idx}')\n",
    "plt.show()\n",
    "\n",
    "ir_recons_plot = np.real(np.fft.ifft2(IR_recons))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction: Compare original and reconstructed ir by listening to the convolved signals\n",
    "\n",
    "# load anechoic sound file\n",
    "x, sr_x=torchaudio.load(\"anechoic.wav\")\n",
    "# resample anechoic signal\n",
    "x=torchaudio.transforms.Resample(sr_x,8e3)(x)\n",
    "x=x.T.numpy()\n",
    "# convolve anechoic signal with RIR: y=x*h\n",
    "y_ir_recons = signal.fftconvolve(x, ir_recons_plot, mode = 'full')\n",
    "y_ir_orig = signal.fftconvolve(x, ir_orig_plot, mode = 'full')\n",
    "# # set signal levels\n",
    "x=helpers.set_level(x.T,-30)\n",
    "y_ir_recons=helpers.set_level(y_ir_recons.T,-30)\n",
    "y_ir_orig=helpers.set_level(y_ir_orig.T,-30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(x,rate=8e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(y_ir_orig,rate=8e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(y_ir_recons,rate=8e3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce963d0b11d3bbec7fb5cbe3b3e5eb22455c53ff6b457d89286ef32149820fa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
